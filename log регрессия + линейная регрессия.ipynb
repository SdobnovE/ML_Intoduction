{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random as rnd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import r2_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Линейная регрессия решает задачу регрессии и оптимизирует функцию потерь MSE \n",
    "\n",
    "$$L(w) =  \\frac{1}{N}\\left[\\sum_i (y_i - a_i) ^ 2 \\right], $$ где $y_i$ $-$ целевая функция,  $a_i = a(x_i) =  \\langle\\,x_i,w\\rangle ,$ $-$ предсказание алгоритма на объекте $x_i$, $w$ $-$ вектор весов (размерности $D$), $x_i$ $-$ вектор признаков (такой же размерности $D$).\n",
    "\n",
    "Будем считать, что в $x_i$ есть тождественный вектор единиц, ему соответствует вес $w_0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логистическая регрессия является линейным классификатором, который оптимизирует функционал log loss:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$L(w) = - \\frac{1}{N}\\left[\\sum_i y_i \\log a_i + ( 1 - y_i) \\log (1 - a_i) \\right],$$\n",
    "где  $y_i  \\in \\{0,1\\}$ $-$ метка класса, $a_i$ $-$ предсказание алгоритма на объекте $x_i$. Модель пытается предсказать апостериорую вероятность объекта принадлежать к классу \"1\":\n",
    "$$ p(y_i = 1 | x_i) = a(x_i) =  \\sigma( \\langle\\,x_i,w\\rangle ),$$\n",
    "$w$ $-$ вектор весов (размерности $D$), $x_i$ $-$ вектор признаков (такой же размерности $D$).\n",
    "\n",
    "Функция $\\sigma(x)$ $-$ нелинейная функция, пероводящее скалярное произведение объекта на веса в число $\\in (0,1)$ (пытаемся моделировать вероятность)\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + \\exp(-x)}$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часто для решения проблем, которые так или иначе связаны с проблемой переобучения, в функционал качества добавляют слагаемое, которое называют ***регуляризацией***. Итоговый функционал для линейной регрессии тогда принимает вид:\n",
    "\n",
    "$$L(w) =  \\frac{1}{N}\\left[\\sum_i (y_i - a_i) ^ 2 \\right] + \\frac{1}{C}R(w) $$\n",
    "\n",
    "Для логистической: \n",
    "$$L(w) = - \\frac{1}{N}\\left[\\sum_i y_i \\log a_i + ( 1 - y_i) \\log (1 - a_i) \\right] +  \\frac{1}{C}R(w)$$\n",
    "\n",
    "Идейно методика регуляризации заключается в следующем $-$ мы рассматриваем некорректно поставленную задачу для того чтобы сузить набор различных вариантов (лучшие из которых будут являться переобучением ) мы вводим дополнительные ограничения на множество искомых решений.\n",
    "\n",
    "$L1$ регуляризация:\n",
    "$$R(w) = \\sum_{j=1}^{D}|w_j|$$\n",
    "$L2$ регуляризация:\n",
    "$$R(w) =  \\sum_{j=1}^{D}w_j^2$$\n",
    "\n",
    "С их помощью мы ограничиваем модель в  возможности выбора каких угодно весов минимизирующих наш лосс, модель уже не сможет подстроиться под данные как ей угодно. \n",
    "\n",
    "\n",
    "\n",
    "И так, мы поняли, какую функцию ошибки будем минимизировать, разобрались, как получить предсказания по объекту и обученным весам. Осталось разобраться, как получить оптимальные веса. Для этого нужно выбрать какой-то метод оптимизации.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Метод оптимизации функции потерь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD градиентный спуск - стохастический градиентный спуск\n",
    "MB-GD - мини батч градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиентный спуск является самым популярным алгоритмом обучения линейных моделей. Далее нам потребуется определение **эпохи**.\n",
    "Эпохой в SGD и MB-GD называется один проход по **всем** объектам в обучающей выборки.\n",
    "* В SGD градиент расчитывается по одному случайному объекту. Сам алгоритм выглядит примерно так:\n",
    "        1) Перемешать выборку\n",
    "        2) Посчитать градиент функции потерь на одном объекте (один объект тоже будем называть батчем)\n",
    "        3) Сделать шаг спуска\n",
    "        4) Повторять 2) и 3) пока не пройдет максимальное число эпох.\n",
    "* В Mini Batch SGD - по подвыборке объектов. Сам алгоритм выглядит примерно так::\n",
    "        1) Перемешать выборку, выбрать размер мини-батча (от 1 до размера выборки)\n",
    "        2) Почитать градиент функции потерь по мини-батчу \n",
    "        3) Сделать шаг спуска\n",
    "        4) Повторять 2) и 3) пока не пройдет максимальное число эпох.\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Формула обновления весов в линейной регрессии с l2 регуляризацией"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ w_{new} = w_{old} - 2\\alpha (\\frac{1}{n}X^{T}(Xw_{old} - Y) + \\lambda w_{old}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Формула обновления весов в логистической регрессии с l2 регуляризацией"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ w_{new} = w_{old} - \\alpha (\\frac{1}{n}X^{T}(V - Y) + 2\\lambda w_{old}) $$\n",
    "\n",
    "\n",
    "\n",
    "$$ V_{i} = \\frac{1}{1+\\exp(-\\langle x_{i}\\cdot w \\rangle)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существует точное выражение для нахождения минимума в методе наименьших квадратов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$w = (X^TX)^{-1}X^TY $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из-за плохой обусловленности матрицы и того, что умножение матриц выполняется за $O(n^3)$ такой метод отыскания минимума не применяют на практике"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(X, y, shuffle=True, batch_size=1):\n",
    "    \"\"\"\n",
    "    Гератор новых батчей для обучения\n",
    "    X          - матрица объекты-признаки\n",
    "    y_batch    - вектор ответов\n",
    "    shuffle    - нужно ли случайно перемешивать выборку\n",
    "    batch_size - размер батча ( 1 это SGD, > 1 mini-batch GD)\n",
    "    Генерирует подвыборку для итерации спуска (X_batch, y_batch)\n",
    "    \"\"\"\n",
    "    \n",
    "    X_batch = \"\"\n",
    "    y_batch = \"\"\n",
    "    num = 0\n",
    "    if shuffle == True:\n",
    "        while True:\n",
    "            ind = rnd.sample([i for i in range(X.shape[0])], batch_size)\n",
    "            \n",
    "            yield (num, (X[ind,:], y[ind]))\n",
    "            num += 1       \n",
    "    else:\n",
    "        i = 0\n",
    "        n = X.shape[0]\n",
    "        while True:\n",
    "            ind = [t % n for t in range(i, i + batch_size)]\n",
    "            i += batch_size\n",
    "            yield (num, (X[ind,:], y[ind]))\n",
    "            num += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Вычисляем значение сигмоиды.\n",
    "    X - выход линейной модели\n",
    "    \"\"\"\n",
    "    \n",
    "    sigm_value_x = 1. / (1 + np.exp(-x))\n",
    "    return sigm_value_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class MySGDLinearModels(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    def __init__(self, batch_generator, C=1, alpha=0.01, \n",
    "                 max_epoch=10, model_type='lin_reg',\n",
    "                batch_size=1):\n",
    "        \"\"\"\n",
    "        batch_generator -- функция генератор, которой будем создавать батчи\n",
    "        C - коэф. регуляризации\n",
    "        alpha - скорость спуска\n",
    "        max_epoch - максимальное количество эпох\n",
    "        model_type - тип модели, lin_reg или log_reg\n",
    "        \"\"\"\n",
    "        if model_type != 'lin_reg' and model_type != 'log_reg':\n",
    "            raise TypeError\n",
    "            \n",
    "        self.batch_size = batch_size   \n",
    "        self.C = C\n",
    "        self.alpha = alpha\n",
    "        self.max_epoch = max_epoch\n",
    "        self.batch_generator = batch_generator\n",
    "        self.errors_log = {'iter' : [], 'loss' : []}  \n",
    "        self.model_type = model_type\n",
    "        \n",
    "    def calc_loss(self, X_batch, y_batch):\n",
    "        \"\"\"\n",
    "        Считаем функцию потерь по батчу \n",
    "        X_batch - матрица объекты-признаки по батчу\n",
    "        y_batch - вектор ответов по батчу\n",
    "        \"\"\"\n",
    "        \n",
    "        loss = 0\n",
    "        if self.model_type == 'log_reg':\n",
    "            p = sigmoid(np.dot(X_batch, self.weights))\n",
    "            for i in p:\n",
    "                if i < 1e-16:\n",
    "                    i = 0.0001\n",
    "            \n",
    "                if 1 - i < 1e-16:\n",
    "                    i = 0.9999\n",
    "                \n",
    "            loss = np.dot(y_batch, np.log(p))\n",
    "            loss += np.dot(1 - y_batch, np.log(1 - p))\n",
    "            loss *= -1\n",
    "            loss /= y_batch.shape[0]\n",
    "        \n",
    "        else:\n",
    "            loss = ((y_batch - np.dot(X_batch, self.weights)) ** 2).sum()\n",
    "            \n",
    "            loss /= y_batch.shape[0]\n",
    "            \n",
    "        return loss \n",
    "    \n",
    "    \n",
    "    def calc_loss_grad(self, X_batch, y_batch):\n",
    "        \"\"\"\n",
    "        Считаем  градиент функции потерь по батчу\n",
    "        X_batch - матрица объекты-признаки по батчу\n",
    "        y_batch - вектор ответов по батчу\n",
    "        \"\"\"\n",
    "        loss_grad = 0\n",
    "        if self.model_type == 'lin_reg':\n",
    "            loss_grad = np.dot(X_batch, self.weights) \n",
    "            loss_grad -= y_batch\n",
    "            loss_grad = np.dot(X_batch.transpose(), loss_grad)\n",
    "            loss_grad /= y_batch.shape[0]\n",
    "            \n",
    "            temp = self.weights[0]\n",
    "            self.weights[0] = 0.\n",
    "            loss_grad += self.C * self.weights\n",
    "            self.weights[0] = temp\n",
    "                \n",
    "            loss_grad *= 2\n",
    "            \n",
    "        else:\n",
    "            V = sigmoid(np.dot(X_batch, self.weights))\n",
    "            loss_grad = V - y_batch\n",
    "            loss_grad = np.dot(X_batch.transpose(), loss_grad)\n",
    "            loss_grad /= y_batch.shape[0]\n",
    "            \n",
    "            temp = self.weights[0]\n",
    "            self.weights[0] = 0.\n",
    "            loss_grad += 2 * self.C * self.weights\n",
    "            self.weights[0] = temp\n",
    "         \n",
    "        return loss_grad\n",
    "    \n",
    "    def update_weights(self, new_grad):\n",
    "        \"\"\"\n",
    "        Обновляем вектор весов\n",
    "        new_grad - градиент по батчу\n",
    "        \"\"\"\n",
    "        self.weights -= new_grad * self.alpha\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Обучение модели\n",
    "        X - матрица объекты-признаки\n",
    "        y - вектор ответов\n",
    "        '''\n",
    "        \n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        ones = np.array([1 for i in range(X.shape[0])]).reshape((-1,1))\n",
    "        X = np.concatenate((ones, X), axis=1)\n",
    "\n",
    "        self.weights = np.random.uniform(-10, 10, X.shape[1])\n",
    "        for n in range(0, self.max_epoch):\n",
    "            new_epoch_generator = self.batch_generator(X, y, True, self.batch_size)\n",
    "            iterator = 0\n",
    "            for batch_num, new_batch in new_epoch_generator:\n",
    "                X_batch = new_batch[0]\n",
    "\n",
    "                y_batch = new_batch[1]\n",
    "                batch_grad = self.calc_loss_grad(X_batch, y_batch)\n",
    "                \n",
    "                self.update_weights(batch_grad)\n",
    "                batch_loss = self.calc_loss(X_batch, y_batch)\n",
    "                \n",
    "                self.errors_log['iter'].append(batch_num)\n",
    "                self.errors_log['loss'].append(batch_loss)\n",
    "\n",
    "                if iterator > X.shape[0] / self.batch_size:\n",
    "                    break\n",
    "                iterator+=1\n",
    "                \n",
    "                \n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Предсказание класса\n",
    "        X - матрица объекты-признаки\n",
    "        '''\n",
    "        y_hat = 0\n",
    "        if self.model_type == \"lin_reg\":\n",
    "            y_hat = np.dot(X, self.weights[1:]) + self.weights[0]\n",
    "        \n",
    "        else:\n",
    "            y_hat = sigmoid(np.dot(X, self.weights[1:]) + self.weights[0])\n",
    "        \n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Линейная регрессия, Бостон датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[\"data\"]\n",
    "X = StandardScaler().fit_transform(X)\n",
    "y = data[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.7 s, sys: 79.7 ms, total: 28.8 s\n",
      "Wall time: 28.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_ans = []\n",
    "for train_index, test_index in KFold(n_splits=10).split(X):\n",
    "    mdl = MySGDLinearModels(batch_generator, \n",
    "                      C=0.01, \n",
    "                      alpha=0.001,\n",
    "                      max_epoch=1000,\n",
    "                      model_type='lin_reg',\n",
    "                      batch_size=10)\n",
    "    X_train = X[train_index,:]\n",
    "    y_train = y[train_index]\n",
    "\n",
    "    X_test = X[test_index,:]\n",
    "    y_test = y[test_index]\n",
    "    \n",
    "    mdl.fit(X_train, y_train)\n",
    "    y_ans += list(mdl.predict(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4423795791315479"
      ]
     },
     "execution_count": 536,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_ans, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.5 ms, sys: 93 µs, total: 23.6 ms\n",
      "Wall time: 10.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_ans = []\n",
    "for train_index, test_index in KFold(n_splits=10).split(X):\n",
    "    mdl = Ridge(1)\n",
    "    X_train = X[train_index,:]\n",
    "    y_train = y[train_index]\n",
    "\n",
    "    X_test = X[test_index,:]\n",
    "    y_test = y[test_index]\n",
    "    mdl.fit(X_train, y_train)\n",
    "#     print(r2_score(mdl.predict(X_train), y_train))\n",
    "    y_ans += list(mdl.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44077611520440463"
      ]
     },
     "execution_count": 538,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_ans, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Видим, на кроссвалидации примерно одинаковое качество"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Логистическая регрессия датасет с раком"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[\"data\"]\n",
    "X = StandardScaler().fit_transform(X)\n",
    "y = data[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReturnClasses(prob, threshold):\n",
    "    classes = []\n",
    "    for i in prob:\n",
    "        \n",
    "        if i > threshold:\n",
    "            classes.append(1)\n",
    "        else:\n",
    "            classes.append(0)\n",
    "            \n",
    "    return classes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/evgeny/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:44: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 46.3 s, sys: 38.5 ms, total: 46.3 s\n",
      "Wall time: 45.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "probs = []\n",
    "for train_index, test_index in KFold(n_splits=10).split(X):\n",
    "    mdl = MySGDLinearModels(batch_generator, \n",
    "                      C=0.01, \n",
    "                      alpha=0.001,\n",
    "                      max_epoch=1000,\n",
    "                      model_type='log_reg',\n",
    "                      batch_size=10)\n",
    "    X_train = X[train_index,:]\n",
    "    y_train = y[train_index]\n",
    "\n",
    "    X_test = X[test_index,:]\n",
    "    y_test = y[test_index]\n",
    "    \n",
    "    mdl.fit(X_train, y_train)\n",
    "#     print(mdl.predict(X_test))\n",
    "    probs += list(mdl.predict(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_ans = ReturnClasses(probs, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9574759945130316"
      ]
     },
     "execution_count": 544,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_ans, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 340 ms, sys: 3.92 ms, total: 344 ms\n",
      "Wall time: 96.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "probs = []\n",
    "for train_index, test_index in KFold(n_splits=10).split(X):\n",
    "    mdl = LogisticRegression(C=1)\n",
    "    X_train = X[train_index,:]\n",
    "    y_train = y[train_index]\n",
    "\n",
    "    X_test = X[test_index,:]\n",
    "    y_test = y[test_index]\n",
    "    \n",
    "    mdl.fit(X_train, y_train)\n",
    "#     print(mdl.predict(X_test))\n",
    "    probs += list(mdl.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ans = ReturnClasses(probs, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9819193324061197"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_ans, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Видим, на кроссвалидации примерно одинаковое качество"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
